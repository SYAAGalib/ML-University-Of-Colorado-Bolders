{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Dx9BqaxkuUb"
   },
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient optimization algorithm used to find the values of parameters (coefficients) of a function that minimizes a cost function. In this notebook, we will implement the SGD algorithm from scratch and apply it to learn custom word embeddings for a mock dataset. This implementation will demonstrate how words can be represented as vectors in a continuous space, capturing semantic relationships between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g96d_tSAkuUc"
   },
   "source": [
    "We begin by initializing the embeddings for each item in the dataset. Each word is represented as a 2-dimensional vector with randomly initialized values between -1 and 1. These vectors will be updated during training to capture the semantic relationships between words. Examine the mock dataset of 5 words and their corresponding randomly initialized embeddings below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HhjafMJfkuUd"
   },
   "outputs": [],
   "source": [
    "# Initialize embeddings - each word is represented as a 2D vector\n",
    "# The values are randomly initialized between -1 and 1\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import sympy as sp\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = {\n",
    "    \"the\": [0.1, -0.2],\n",
    "    \"cat\": [-0.3, 0.4],\n",
    "    \"sat\": [0.5, 0.1],\n",
    "    \"on\": [-0.1, 0.3],\n",
    "    \"mat\": [0.2, -0.4]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7o6MDRZlXL5"
   },
   "source": [
    "\n",
    "**Activation Function: Sigmoid**\n",
    "\n",
    "The sigmoid function is a crucial component in our implementation that serves several purposes:\n",
    "\n",
    "1. **Range Compression**: It maps any real number into a value between 0 and 1\n",
    "   σ(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "2. **Probability Interpretation**: In our context, we use sigmoid to convert dot products of word vectors into probabilities. This helps us model the likelihood of words occurring together.\n",
    "\n",
    "3. **Properties**:\n",
    "   - Output is always between 0 and 1\n",
    "   - Differentiable, which is essential for gradient descent\n",
    "   - S-shaped curve that provides non-linear transformation\n",
    "   - Symmetric around 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ruk1QFkFkuUd"
   },
   "outputs": [],
   "source": [
    "# Sigmoid function - used to convert dot products into probabilities\n",
    "# The function maps any real number to a value between 0 and 1\n",
    "def sigmoid(x):\n",
    "\n",
    "      return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xF5KRyxkuUd"
   },
   "source": [
    "**Loss Function**\n",
    "\n",
    "Our implementation uses a specialized loss function designed for word embeddings that combines:\n",
    "\n",
    "1. **Positive Sampling Loss**: For words that appear together in context\n",
    "   -log(σ(v_t · v_c))\n",
    "   where:\n",
    "   - v_t is the target word vector\n",
    "   - v_c is the context word vector\n",
    "   - · denotes dot product\n",
    "   - σ is the sigmoid function\n",
    "\n",
    "2. **Negative Sampling Loss**: For randomly selected negative examples\n",
    "   -log(σ(-v_t · v_n))\n",
    "   where v_n is the negative sample word vector\n",
    "\n",
    "The complete loss function is:\n",
    "L = -log(σ(v_t · v_c)) - log(σ(-v_t · v_n))\n",
    "\n",
    "This loss function:\n",
    "- Maximizes the probability of word pairs that occur together in the text\n",
    "- Minimizes the probability of random word pairs (negative samples)\n",
    "- Is differentiable, allowing us to use gradient descent for optimization\n",
    "- Helps learn word embeddings that capture semantic relationships\n",
    "\n",
    "The negative sampling approach is computationally efficient compared to calculating probabilities over the entire vocabulary, making it practical for large-scale word embedding training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHZZ548YkuUe"
   },
   "source": [
    "Let us now find the derivative of the loss function using symbolic mathematics (sympy). The loss function has three components: the target word embedding (v_t), the context word embedding (v_c), and the negative sample embedding (v_n). We compute the partial derivatives with respect to each of these components to determine how to update the embeddings during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j9MuRBMpkuUe",
    "outputId": "7210e20d-a330-4cbd-a369-9b621cf0ce16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative w.r.t v_t :  v_c*exp(v_c*v_t)/(exp(v_c*v_t) + 1) - v_n*exp(-v_n*v_t)/(1 + exp(-v_n*v_t))\n",
      "Derivative w.r.t v_c :  v_t*exp(v_c*v_t)/(exp(v_c*v_t) + 1)\n",
      "Derivative w.r.t v_n :  -v_t*exp(-v_n*v_t)/(1 + exp(-v_n*v_t))\n"
     ]
    }
   ],
   "source": [
    "v_t = sp.symbols('v_t')\n",
    "v_c = sp.symbols('v_c')\n",
    "v_n = sp.symbols('v_n')\n",
    "\n",
    "f = -sp.log(1 / (1 + sp.exp(v_t*v_c))) - sp.log(1 / (1 + sp.exp(-v_t*v_n)))\n",
    "\n",
    "print(\"Derivative w.r.t v_t : \", sp.diff(f, v_t))\n",
    "print(\"Derivative w.r.t v_c : \", sp.diff(f, v_c))\n",
    "print(\"Derivative w.r.t v_n : \", sp.diff(f, v_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "c-1TLY3xkuUe"
   },
   "outputs": [],
   "source": [
    "def update_embeddings(target, context, negative, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Updates word embeddings using stochastic gradient descent.\n",
    "\n",
    "    The function implements the following steps:\n",
    "    1. Computes dot products between target-context and target-negative pairs\n",
    "    2. Calculates loss using the formula: -log(σ(v_t · v_c)) - log(σ(-v_t · v_n))\n",
    "    3. Computes gradients for all three vectors\n",
    "    4. Updates the embeddings using the computed gradients\n",
    "\n",
    "    Parameters:\n",
    "    - target: The target word whose embedding is being updated\n",
    "    - context: The context word that appears near the target\n",
    "    - negative: A randomly sampled word used as a negative example\n",
    "    - alpha: Learning rate for gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - loss: The current value of the loss function\n",
    "    \"\"\"\n",
    "\n",
    "    v_target = np.array(embeddings[target])\n",
    "    v_context = np.array(embeddings[context])\n",
    "    v_negative = np.array(embeddings[negative])\n",
    "\n",
    "    dot_product_target_context = np.dot(v_target, v_context)\n",
    "    dot_product_target_negative = np.dot(v_target, v_negative)\n",
    "\n",
    "    loss = -np.log(sigmoid(dot_product_target_context)) - np.log(sigmoid(-dot_product_target_negative))\n",
    "    \n",
    "    # FIX: Define sigma_pos and sigma_neg before they are used in the gradients\n",
    "    sigma_pos = sigmoid(dot_product_target_context)\n",
    "    sigma_neg = sigmoid(-dot_product_target_negative)\n",
    "\n",
    "    grad_target = v_context * (sigma_pos - 1) + v_negative * (sigma_neg - 1)\n",
    "    grad_context = v_target * (sigma_pos - 1)\n",
    "    grad_negative = v_target * (1 - sigma_neg)\n",
    "\n",
    "    # TODO: Update embeddings\n",
    "\n",
    "    embeddings[target] = v_target - alpha * grad_target\n",
    "    embeddings[context] = v_context - alpha * grad_context\n",
    "    embeddings[negative] = v_negative - alpha * grad_negative\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4aj7MK9l5lJ"
   },
   "source": [
    "Now we'll train our word embeddings using SGD. We define word pairs based on words that appear together in our mock dataset. For simplicity, we use a fixed negative sample, though in practice this would be randomly sampled. The training process runs for 100 epochs, with the word pairs shuffled in each epoch to improve convergence. We print the loss every 10 epochs to monitor the training progress.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gWxx0oTjkuUe",
    "outputId": "28cb4c82-d5bb-411d-cb8f-cb2cc01e5b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.0981\n",
      "Epoch 10, Loss: 7.0751\n",
      "Epoch 20, Loss: 7.0533\n",
      "Epoch 30, Loss: 7.0317\n",
      "Epoch 40, Loss: 7.0090\n",
      "Epoch 50, Loss: 6.9848\n",
      "Epoch 60, Loss: 6.9602\n",
      "Epoch 70, Loss: 6.9336\n",
      "Epoch 80, Loss: 6.9066\n",
      "Epoch 90, Loss: 6.8788\n",
      "Final embeddings: {'the': array([-0.02048059, -0.02296681]), 'cat': array([-0.18087267,  0.58649518]), 'sat': array([0.23673144, 0.5052219 ]), 'on': array([-0.17716375,  0.0030742 ]), 'mat': array([ 0.06394589, -0.28126674])}\n"
     ]
    }
   ],
   "source": [
    "# Word pairs and negative sampling\n",
    "word_pairs = [(\"cat\", \"the\"), (\"sat\", \"cat\"), (\"on\", \"sat\"), (\"mat\", \"on\"), (\"the\", \"mat\")]\n",
    "negative_sample = \"on\"\n",
    "\n",
    "# Run SGD for 100 epochs\n",
    "for epoch in range(100):\n",
    "    random.shuffle(word_pairs)  # Shuffle word pairs\n",
    "    total_loss = 0\n",
    "    for target, context in word_pairs:\n",
    "        loss = update_embeddings(target, context, negative_sample)\n",
    "        total_loss += loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Final embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geIjjm27kuUf"
   },
   "source": [
    "If you are wondering what these embeddings are, they are a way to represent words in a vector space. These embeddings are learned by the model during training and are used to represent words in a way that captures their meaning and relationships with other words. For example, words that are similar in meaning will have embeddings that are close to each other in the vector space.\n",
    "\n",
    "As a demonstration, let us use pre-trained embeddings from the Word2Vec model to find the embeddings for the words in our mock dataset. We will then compare these embeddings with the embeddings learned by our model. The Word2Vec model is trained on a far bigger dataset and is expected to have better embeddings than the ones learned by our model.\n",
    "\n",
    "To visualize these embeddings, you could:\n",
    "1. Plot them as points in a 2D space since they are 2-dimensional vectors\n",
    "2. Calculate and compare the cosine similarities between different word pairs\n",
    "3. Use dimensionality reduction techniques like t-SNE or PCA if working with higher-dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
